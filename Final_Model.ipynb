{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a5123fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.16.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b477834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.5'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4299baae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "283092d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PUTTING ALL TEXT FROM DIFF DOCS INTO ONE FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b412b1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_file(file_paths, output_file):\n",
    "    with open(output_file, 'w') as outfile:\n",
    "        for file_path in file_paths:\n",
    "            with open(file_path, 'r') as infile:\n",
    "                for line in infile:\n",
    "                    outfile.write(line)\n",
    "\n",
    "# List of file paths\n",
    "file_paths = [\".\\Training Data\\dataset1.txt\", '.\\Training Data\\dataset2_test_output.txt', '.\\Training Data\\dataset3_small_1.txt']\n",
    "\n",
    "# Output file\n",
    "output_file = 'all_data_combined.txt'\n",
    "\n",
    "# Create train.txt file with concatenated text\n",
    "create_train_file(file_paths, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6fd0bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SENDING THAT TRAIN FILE AS INPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2e0b6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_table('all_data_combined.txt', delimiter = ';', header=None, )\n",
    "val=pd.read_table('val.txt', delimiter = ';', header=None, )\n",
    "test=pd.read_table('test.txt', delimiter = ';', header=None, )\n",
    "data = pd.concat([train ,  val , test])\n",
    "data.columns = [\"text\", \"label\"]\n",
    "\n",
    "# val=pd.read_table('val.txt', delimiter = ';', header=None, )\n",
    "# test=pd.read_table('test.txt', delimiter = ';', header=None, )\n",
    "# data = pd.concat([train ,  val , test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c44756fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape # previously 20000 data now 452809   (dataset1 + dataset2 = (36000, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3042a2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().any(axis=1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6eac3269",
   "metadata": {},
   "outputs": [],
   "source": [
    "#text preprocessing\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def preprocess(line):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', line) #not alphabets it replaces with space\n",
    "    review = review.lower() #lower the text\n",
    "    review = review.split() #turn string into list of words\n",
    "    #apply Stemming \n",
    "    review = [ps.stem(word) for word in review if not word in stopwords.words('english')] #delete stop words like I, and ,OR   review = ' '.join(review)\n",
    "    #turn list into sentences\n",
    "    return \" \".join(review)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0b585db",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data['text']=data['text'].apply(lambda x: preprocess(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32480b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "data['N_label'] = label_encoder.fit_transform(data['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8fea19",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596e2847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Bag of Words model by applying Countvectorizer -convert textual data to numerical data\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(max_features=5000,ngram_range=(1,3))\n",
    "\n",
    "data_cv = cv.fit_transform(data['text']).toarray()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273c3e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d54b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test =train_test_split(data_cv, data['N_label'], test_size=0.25, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "33985ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kumar_lf3uub3\\Documents\\python\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:85: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m7500/7500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 3ms/step - accuracy: 0.7283 - loss: 0.7587\n",
      "Epoch 2/10\n",
      "\u001b[1m7500/7500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 4ms/step - accuracy: 0.9185 - loss: 0.2150\n",
      "Epoch 3/10\n",
      "\u001b[1m7500/7500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 4ms/step - accuracy: 0.9380 - loss: 0.1593\n",
      "Epoch 4/10\n",
      "\u001b[1m7500/7500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 3ms/step - accuracy: 0.9491 - loss: 0.1293\n",
      "Epoch 5/10\n",
      "\u001b[1m7500/7500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 3ms/step - accuracy: 0.9585 - loss: 0.1058\n",
      "Epoch 6/10\n",
      "\u001b[1m7500/7500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 3ms/step - accuracy: 0.9632 - loss: 0.0930\n",
      "Epoch 7/10\n",
      "\u001b[1m7500/7500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 4ms/step - accuracy: 0.9680 - loss: 0.0797\n",
      "Epoch 8/10\n",
      "\u001b[1m7500/7500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 4ms/step - accuracy: 0.9711 - loss: 0.0691\n",
      "Epoch 9/10\n",
      "\u001b[1m7500/7500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 3ms/step - accuracy: 0.9746 - loss: 0.0628\n",
      "Epoch 10/10\n",
      "\u001b[1m7500/7500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 4ms/step - accuracy: 0.9771 - loss: 0.0555\n",
      "\u001b[1m2344/2344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.9804 - loss: 0.0476\n",
      "Accuracy: 98.14\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_shape=(X_train.shape[1],), activation='relu')) # Rectified Linear Unit \n",
    "model.add(Dense(8, activation='relu')) # Returns zero if its negativate and returns as it is if value is positive\n",
    "model.add(Dense(6, activation='softmax')) #used for classification problems\n",
    "#  For multi-class classification problems where the labels are integers\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit the keras model on the dataset\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=10)\n",
    "# evaluate the keras model\n",
    "_, accuracy = model.evaluate(X_train, y_train)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b6867f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.8997 - loss: 0.4511\n",
      "Accuracy: 90.10\n"
     ]
    }
   ],
   "source": [
    "_, accuracy = model.evaluate(X_test, y_test)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fee946e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'sadness'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "text='I feel sad'\n",
    "text=preprocess(text)\n",
    "array = cv.transform([text]).toarray()\n",
    "pred = model.predict(array)\n",
    "a=np.argmax(pred, axis=1) \n",
    "label_encoder.inverse_transform(a)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79d9352",
   "metadata": {},
   "source": [
    "# Emotion Detector For Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ecffd7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'anger'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def emotion_for_text(text):\n",
    "    text=preprocess(text)\n",
    "    array = cv.transform([text]).toarray()\n",
    "    pred = model.predict(array)\n",
    "    a=np.argmax(pred, axis=1) #index of the highest probability prediction is extracted \n",
    "    return label_encoder.inverse_transform(a)[0]\n",
    "    \n",
    "emotion_for_text('I hate you')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7397ae7",
   "metadata": {},
   "source": [
    "# Emotion Detector For File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9b5836bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Input Text: hello, I love you\n",
      "Predicted Label: love\n",
      "Accuracy: 90.10\n"
     ]
    }
   ],
   "source": [
    "def emotion_for_file(file_path):\n",
    "    # Open the text file\n",
    "    with open(file_path, 'r') as file:\n",
    "        # Iterate over each line in the file\n",
    "        for line in file:\n",
    "            # Preprocess the line\n",
    "            preprocessed_text = preprocess(line)\n",
    "            # Transform preprocessed text into numerical representation\n",
    "            input_array = cv.transform([preprocessed_text]).toarray()\n",
    "            # Get predictions from the model\n",
    "            predictions = model.predict(input_array)\n",
    "            # Convert predictions to labels\n",
    "            predicted_label_index = np.argmax(predictions, axis=1)\n",
    "            predicted_label = label_encoder.inverse_transform(predicted_label_index)[0]\n",
    "            # Print the predicted label for the current line\n",
    "            print(\"Input Text:\", line.strip())1\n",
    "            print(\"Predicted Label:\", predicted_label)\n",
    "            print('Accuracy: %.2f' % (accuracy*100))\n",
    "\n",
    "# Test the model on a text file\n",
    "emotion_for_file('sample_text.txt')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fdaaec",
   "metadata": {},
   "source": [
    "# Comparing Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8988f52d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Classes:\n",
      "anger\n",
      "fear\n",
      "joy\n",
      "love\n",
      "sadness\n",
      "surprise\n"
     ]
    }
   ],
   "source": [
    "classes = label_encoder.classes_\n",
    "\n",
    "# Print all the classes\n",
    "print(\"All Classes:\")\n",
    "for cls in classes:\n",
    "    print(cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9a39f537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracies for all classes\n",
    "def accuracy_for_all_classes(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        sample_text = file.read()\n",
    "\n",
    "    # Preprocess the text\n",
    "    preprocessed_text = preprocess(sample_text)\n",
    "\n",
    "    # Transform preprocessed text into numerical representation\n",
    "    input_array = cv.transform([preprocessed_text]).toarray()\n",
    "\n",
    "    # Get predictions from the model\n",
    "    predictions = model.predict(input_array)\n",
    "\n",
    "    # Count occurrences of each class label\n",
    "    class_counts = np.sum(predictions, axis=0)\n",
    "\n",
    "    # Calculate percentages\n",
    "    total_predictions = len(predictions)\n",
    "    class_percentages = class_counts / total_predictions * 100\n",
    "\n",
    "    # Get class labels\n",
    "    classes = label_encoder.classes_\n",
    "\n",
    "    # Print percentages for all classes\n",
    "    print(\"Class Percentages for the Sample Text:\")\n",
    "    for cls, percentage in zip(classes, class_percentages):\n",
    "        print(f\"{cls}: {percentage:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5877746b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "tf.keras.models.save_model(model,'my_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ea79609f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(label_encoder, open('encoder.pkl', 'wb'))\n",
    "pickle.dump(cv, open('CountVectorizer.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99808247",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "57159837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Input Text: hello, I love you\n",
      "Predicted Label: love\n",
      "Accuracy: 90.10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Class Percentages for the Sample Text:\n",
      "anger: 0.38%\n",
      "fear: 4.11%\n",
      "joy: 28.46%\n",
      "love: 58.98%\n",
      "sadness: 7.87%\n",
      "surprise: 0.19%\n"
     ]
    }
   ],
   "source": [
    "# Test the model on a text file\n",
    "emotion_for_file('sample_text.txt')\n",
    "accuracy_for_all_classes('sample_text.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c4e7fbd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8997 - loss: 0.4511\n",
      "Loss: 0.44759538769721985\n",
      "Accuracy: 0.9009600281715393\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test dataset\n",
    "loss, accuracy = loaded_model.evaluate(X_test, y_test)\n",
    "\n",
    "print(\"Loss:\", loss)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a39daf",
   "metadata": {},
   "source": [
    "# Loading The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1b356934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">60,012</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">104</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m)                  │          \u001b[38;5;34m60,012\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)                   │             \u001b[38;5;34m104\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)                   │              \u001b[38;5;34m54\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">60,170</span> (235.04 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m60,170\u001b[0m (235.04 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">60,170</span> (235.04 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m60,170\u001b[0m (235.04 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(loaded_model.summary()) # before loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3514d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">60,012</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">104</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m)                  │          \u001b[38;5;34m60,012\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)                   │             \u001b[38;5;34m104\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)                   │              \u001b[38;5;34m54\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">60,170</span> (235.04 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m60,170\u001b[0m (235.04 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">60,170</span> (235.04 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m60,170\u001b[0m (235.04 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Dense name=dense_3, built=True>, <Dense name=dense_4, built=True>, <Dense name=dense_5, built=True>]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "\n",
    "# loading\n",
    "model = tf.keras.models.load_model('my_model.h5')\n",
    "\n",
    "# Recompile the model with the same settings\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# # Save the model again\n",
    "# tf.keras.models.save_model(model, 'my_model_updated.h5')\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\n",
    "# Alternatively, you can inspect the model architecture\n",
    "print(model.layers)\n",
    "\n",
    "# Load the label encoder\n",
    "with open('encoder.pkl', 'rb') as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "\n",
    "# Load the CountVectorizer\n",
    "with open('CountVectorizer.pkl', 'rb') as f:\n",
    "    cv = pickle.load(f)\n",
    "\n",
    "    #model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4be9abd",
   "metadata": {},
   "source": [
    "# Evaluation With Test Data And With 'Model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e61a2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19968737",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "                                                   Text Original Predicted\n",
      "0                  im feel quit sad sorri ill snap soon  sadness   sadness\n",
      "1     feel like still look blank canva blank piec paper  sadness   sadness\n",
      "2                               feel like faith servant     love      love\n",
      "3                                      feel cranki blue    anger     anger\n",
      "4                                     treat feel festiv      joy       joy\n",
      "...                                                 ...      ...       ...\n",
      "1995  im ssa examin tomorrow morn im quit well prepa...  sadness   sadness\n",
      "1996  constantli worri fight natur push limit inner ...      joy       joy\n",
      "1997                feel import share info experi thing      joy       joy\n",
      "1998  truli feel passion enough someth stay true suc...      joy       joy\n",
      "1999   feel like wanna buy cute make see onlin even one      joy       joy\n",
      "\n",
      "[2000 rows x 3 columns]\n",
      "Accuracy: 0.958\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "\n",
    "# Load the test data\n",
    "test_data = pd.read_table('val.txt', delimiter=';', header=None)\n",
    "test_data.columns = [\"text\", \"actual_emotion\"]\n",
    "\n",
    "# Load the encoder\n",
    "with open(\"encoder.pkl\", \"rb\") as encoder_file:\n",
    "    loaded_encoder = pickle.load(encoder_file)\n",
    "\n",
    "# Load the CountVectorizer\n",
    "with open(\"CountVectorizer.pkl\", \"rb\") as cv_file:\n",
    "    loaded_cv = pickle.load(cv_file)\n",
    "\n",
    "# Preprocess the test data\n",
    "def preprocess_test_data(text):\n",
    "    ps = PorterStemmer()\n",
    "    review = re.sub('[^a-zA-Z]', ' ', text) # leave only characters from a to z\n",
    "    review = review.lower() # lower the text\n",
    "    review = review.split() # turn string into list of words\n",
    "    review = [ps.stem(word) for word in review if not word in stopwords.words('english')] # delete stop words like I, and, OR\n",
    "    return \" \".join(review)\n",
    "\n",
    "test_data['text'] = test_data['text'].apply(preprocess_test_data)\n",
    "\n",
    "# Transform text data into numerical representation using CountVectorizer\n",
    "X_test = loaded_cv.transform(test_data['text']).toarray()\n",
    "\n",
    "# Load the trained model\n",
    "loaded_model = load_model(\"my_model.h5\")\n",
    "\n",
    "# Make predictions\n",
    "predictions = loaded_model.predict(X_test)\n",
    "\n",
    "# Convert predictions to labels\n",
    "predicted_label_indices = np.argmax(predictions, axis=1)\n",
    "predicted_labels = loaded_encoder.inverse_transform(predicted_label_indices)\n",
    "\n",
    "# Calculate accuracy\n",
    "actual_labels = test_data['actual_emotion'].values\n",
    "accuracy = np.mean(predicted_labels == actual_labels)\n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "results_df = pd.DataFrame({\n",
    "    'Text': test_data['text'],\n",
    "    'Original': test_data['actual_emotion'],\n",
    "    'Predicted': predicted_labels\n",
    "})\n",
    "\n",
    "# Print the DataFrame\n",
    "print(results_df)\n",
    "# Print accuracy\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3301fa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d3e93c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "                                                   Text Original Predicted\n",
      "0                 im feel rather rotten im ambiti right  sadness   sadness\n",
      "1                             im updat blog feel shitti  sadness   sadness\n",
      "2            never make separ ever want feel like asham  sadness   sadness\n",
      "3     left bouquet red yellow tulip arm feel slightl...      joy       joy\n",
      "4                                   feel littl vain one  sadness   sadness\n",
      "...                                                 ...      ...       ...\n",
      "1995  keep feel like someon unkind wrong think get b...    anger     anger\n",
      "1996            im feel littl cranki neg doctor appoint    anger     anger\n",
      "1997              feel use peopl give great feel achiev      joy       joy\n",
      "1998  im feel comfort derbi feel though start step s...      joy       joy\n",
      "1999  feel weird meet w peopl text like dont talk fa...     fear      fear\n",
      "\n",
      "[2000 rows x 3 columns]\n",
      "Accuracy: 0.947\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "\n",
    "# Load the test data\n",
    "test_data = pd.read_table('test.txt', delimiter=';', header=None)\n",
    "test_data.columns = [\"text\", \"actual_emotion\"]\n",
    "\n",
    "# Load the encoder\n",
    "with open(\"encoder.pkl\", \"rb\") as encoder_file:\n",
    "    loaded_encoder = pickle.load(encoder_file)\n",
    "\n",
    "# Load the CountVectorizer\n",
    "with open(\"CountVectorizer.pkl\", \"rb\") as cv_file:\n",
    "    loaded_cv = pickle.load(cv_file)\n",
    "\n",
    "# Preprocess the test data\n",
    "def preprocess_test_data(text):\n",
    "    ps = PorterStemmer()\n",
    "    review = re.sub('[^a-zA-Z]', ' ', text) # leave only characters from a to z\n",
    "    review = review.lower() # lower the text\n",
    "    review = review.split() # turn string into list of words\n",
    "    review = [ps.stem(word) for word in review if not word in stopwords.words('english')] # delete stop words like I, and, OR\n",
    "    return \" \".join(review)\n",
    "\n",
    "test_data['text'] = test_data['text'].apply(preprocess_test_data)\n",
    "\n",
    "# Transform text data into numerical representation using CountVectorizer\n",
    "X_test = loaded_cv.transform(test_data['text']).toarray()\n",
    "\n",
    "# Load the trained model\n",
    "loaded_model = load_model(\"my_model.h5\")\n",
    "\n",
    "# Make predictions\n",
    "predictions = loaded_model.predict(X_test)\n",
    "\n",
    "# Convert predictions to labels\n",
    "predicted_label_indices = np.argmax(predictions, axis=1)\n",
    "predicted_labels = loaded_encoder.inverse_transform(predicted_label_indices)\n",
    "\n",
    "# Calculate accuracy\n",
    "actual_labels = test_data['actual_emotion'].values\n",
    "accuracy = np.mean(predicted_labels == actual_labels)\n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "results_df = pd.DataFrame({\n",
    "    'Text': test_data['text'],\n",
    "    'Original': test_data['actual_emotion'],\n",
    "    'Predicted': predicted_labels\n",
    "})\n",
    "\n",
    "# Print the DataFrame\n",
    "print(results_df)\n",
    "# Print accuracy\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96a3275",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
